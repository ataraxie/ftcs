\subsection{Replication}
\label{sec:replication}

Fault tolerance and reliability in distributed systems with client-server architecture are generally achieved by data replication: information is shared on redundant server replicas such that any replica can become the new master if the current master fails. While improving system artifacts like fault-tolerance, reliability and availability, replication can come at the cost of performance: depending on the required operations in the system for replication, system performance can suffer significant bottlenecks. Different models of replication have been proposed to trade consistency for performance, resulting in different levels of consistency as a design choice for the target system.

\textbf{Active vs. passive replication.} Traditionally, two strategies of replication are distinguished: \textit{active replication} and \textit{passive replication}. In \textbf{active replication} (also called \textit{primary-backup} or \textit{master-slave}), requests to the master replica are processed to all other replicas. Given the same initial state and request sequence, all replicas will produce the same response sequence and reach the same final state. Active replication has been become most prominent with the introduction of the State Machine Replication model which was introduced in the 1980s \cite{Lamport:1984} and later refined in \cite{Schneider:1990}. It is based on the concept of distributed consensus with the goal of reliably reaching a stable state of the system in the presence of failures. While providing small recovery delay after failures due to an imposed total order of state updates, computation performance can suffer tremendous bottlenecks since updates must be sequentially propagated through all replicas. The second strategy of \textbf{passive replication} (also called multi-primary or multi-master scheme) potentially improves computation performance by relaxing sequential ordering: clients communicate with a master replica and updates are forwarded to backup replicas. Computation performance is improved with this pattern since all computation takes place on the master replica and only the results are propagated. The downside of the approach is that more network bandwidth is required if updates are large. Since the primary replica represents a single point of entry to clients with this approach, there must be some kind of distributed concurrency control in order to reliably restore state when the primary fails. This makes the implementation of this approach more complex and recovery potentially slower.

\textbf{Lazy replication.} A third strategy of replication was proposed in 1990: \textit{lazy replication} \cite{Ladin:1990,Ladin:1992} (also called \textit{optimistic replication}) aims at providing highest possible performance by sacrificing consistency significantly. With this approach, replicas periodically exchange information, tolerating out-of-sync periods but guarantee to catch up eventually. While the traditional approaches guarantee from the beginning that all replicas have the exact same state at any point in time, lazy replication allows states to diverge on replicas, but guarantees that the states converge when the system quiesces for some time period. In contrast to the strong consistency models used in the traditional approaches, lazy replication is based on eventual consistency which has gained more attention recently, in particular with the introduction of \textit{conflict-free replicated data types} \cite{Shapiro:2011}, online editing platforms and NoSQL cloud databases \footnote{https://cloud.google.com/datastore/docs/articles/balancing-strong-and-eventual-consistency-with-google-cloud-datastore/} which rely on immediate response for general usability.

\textbf{Eventual consistency and conflict-free replicated data types.}

 
